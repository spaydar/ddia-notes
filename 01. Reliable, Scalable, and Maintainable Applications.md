# 1. Reliable, Scalable, and Maintainable Applications
- Commonly needed functionality of data-intensive applications:
  - Databases
    - Store data so it can be found again later
  - Caches
    - Remember the result of an expensive operation to speed up reads
  - Search Indexes
    - Allow users to search data by keyword or filter it in various ways
  - Stream Processing
    - Send a message to another process, to be handled asynchronously
  - Batch Processing
    - Periodically crunch a large amount of accumulated data
- Three concerns that are important in most software systems:
  - Reliability
    - The system should continue to work _correctly_ 
      - performing the correct function at the desired level of performance 
    - even in the face of _adversity_
      - hardware or software faults, and even human error
  - Scalability
    - As the system _grows_,
      - in data volume, traffic volume, or complexity
    - there should be reasonable ways of dealing with that growth
  - Maintainability
    - Over time, the many people who work on the system should be able to do so _productively_
      - engineering and operations
      - maintaining current behavior and adapting the system to new use cases
## Reliability
- Software reliability typically means:
  - The application performs the function that the user expected
  - It can tolerate the user making mistakes or using the software in unexpected ways
  - Its performance is good enough for the required use case, under the expected load and data volume
  - The system prevents any unauthorized access and abuse
- Faults
  - Things that can go wrong
  - One component of a system deviating from its spec
- Failure
  - When a system as a whole stops providing the required service to the user
- It is usually best to design fault-tolerance mechanisms that prevent faults from causing failures
  - This is because it is impossible to reduce the probability of a fault to zero
- It can make sense to deliberately induce faults 
  - to ensure fault-tolerance machinery is continually exercised and tested
  - e.g. the Netflix _Chaos Monkey_
### Hardware Faults
- Hard disk crash, faulty RAM, power grid blackout, unplug wrong network cable, etc.
  - Can be mitigated with redundant hardware components and backup power
- There is a move toward systems that can tolerate the loss of entire machines because:
  - Increased data volumes and computing demands over time means more applications use larger numbers of machines 
    - this proportionally increases the rate of hardware faults
  - Some cloud platforms are designed to prioritize flexibility and elasticity over single-machine reliability
    - thus it is fairly common for VM instances to become unavailable without warning
- Such systems
  - are achieved using software fault-tolerance techniques in preference or in addition to hardware redundancy
  - have operational advantages like the ability to do a rolling upgrade
### Software Errors
- System failures are more commonly rooted in software faults than in hardware faults
  - Hardware faults are uncorrelated or weakly correlated
    - e.g. Temperature in server rack
  - Software faults are correlated across nodes and are harder to anticipate
- Software faults often lie dormant for a long time until an unusual set of circumstances exposes them
  - The circumstances reveal a usually-true assumption the software made about its environment stopped being true
- Software fault examples:
    - OS kernel bug
    - Runaway process that uses up a shared resource
    - A vital service degrades
    - Cascading failures
- No quick solution exists, but mitigation can include:
  - Careful consideration of assumptions and interactions in the system
  - Thorough testing
  - Process isolation
  - Allowing processes to crash and restart
  - Measuring, monitoring, and analyzing system behavior in production
### Human Errors
- Humans are unreliable
  - Configuration errors by human operators are the leading cause of outages of large internet services
- How to make systems reliable in spite of unreliable humans:
  - Design systems that minimize opportunities for error
    - Well-designed interfaces make "the right thing" easy and discourage "the wrong thing"
      - Don't make them too restrictive, otherwise people will work around them
  - Utilize _sandbox_ environments to experiment safely with real data without affecting real users
  - Test thoroughly
    - Unit, integration, manual, and automated tests
  - Enable quick and easy recovery from human errors to minimize impact
    - Fast configuration rollback, gradual rollout of new code, tools to recompute data
  - Set up _telemetry_
    - i.e. detailed and clear monitoring of performance metrics and error rates
  - Implement good management practices and training
## Scalability
